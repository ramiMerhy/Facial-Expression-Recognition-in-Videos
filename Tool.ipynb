{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading\n",
      "start recording\n",
      "Read a new frame:  True\n",
      "face found\n",
      "1\n",
      "Read a new frame:  True\n",
      "face found\n",
      "2\n",
      "Read a new frame:  True\n",
      "face found\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmerhy\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5122,  3.9617,  0.1327, -0.6645, -2.8542, -0.2641, -0.5265]])\n",
      "[[0.00394759 0.9410266  0.02045012 0.00921435 0.00103158 0.01375138\n",
      "  0.01057824]]\n",
      "1\n",
      "Angry\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import math\n",
    "init=[]\n",
    "framesave=[]\n",
    "peporcessing=[]\n",
    "net=[]\n",
    "totaltime=[]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Network definition; convolution process\n",
    "#in_planes: input size; out_planes: output size; kernel_size: the mid layer (constructed by the W); stride: nb of skipped pixels; padding: how to treat the missing pixels; bias: constant values that we add or not\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "#define a block\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "#block: convolution then batch normalization then relu\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    #residual block equation\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#define the structure\n",
    "class ResNet_AT(nn.Module):\n",
    "    def __init__(self, block, layers,featureVectoreSize):\n",
    "        super(ResNet_AT, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        #convolution then batch normalization then relu without creating a block\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        #max pooling: set the kernel and get the max value\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        #defining the layers (64,128,256 are the output size)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, featureVectoreSize, layers[3], stride=2)\n",
    "        #get the average pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier=nn.Linear(512,7)\n",
    "        #initials weights based on a normal distribution\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    #creating layers and each layer has some blocks then make sure that input and output size of consecutive blocks are equal\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.size())\n",
    "        #change dimention order\n",
    "        x=x.permute(0,4,1,2,3)\n",
    "        #check each dimension size\n",
    "        batch_size, seqlen, nc, h, w = x.size()\n",
    "        #combine the batch size and sequence lenght together\n",
    "        x = x.reshape(-1, nc, h, w)\n",
    "\n",
    "        #apply all the above to get the result (emotion)\n",
    "        f = self.conv1(x)\n",
    "        f = self.bn1(f)\n",
    "        f = self.relu(f)\n",
    "        f = self.maxpool(f)\n",
    "        f = self.layer1(f)\n",
    "        f = self.layer2(f)\n",
    "        f = self.layer3(f)\n",
    "        f = self.layer4(f)\n",
    "        f = self.avgpool(f)\n",
    "\n",
    "        #reduce the 30 values to 10 (each value has 3 emotion scores)\n",
    "        out = f.reshape(batch_size, seqlen, -1)\n",
    "        #average the result on the 3 images from the same video\n",
    "        out=out.mean(1)\n",
    "        #\n",
    "        out=self.classifier(out)\n",
    "        return out\n",
    "\n",
    "#allow to load the weights of the model\n",
    "def model_parameters(_structure, _parameterDir):\n",
    "    #checkpoint: a python dictionary where the weights are saved\n",
    "    checkpoint = torch.load(_parameterDir)\n",
    "    #state_dict: are the weights\n",
    "    pretrained_state_dict = checkpoint['state_dict']\n",
    "    model_state_dict = _structure.state_dict()\n",
    "\n",
    "    for key in pretrained_state_dict:\n",
    "        if ((key == 'module.fc.weight') | (key == 'module.fc.bias')):\n",
    "            pass\n",
    "        else:\n",
    "            model_state_dict[key.replace('module.', '')] = pretrained_state_dict[key]\n",
    "\n",
    "    #loading the weights\n",
    "    _structure.load_state_dict(model_state_dict)\n",
    "    #load them on the gpu if exists\n",
    "    if torch.cuda.is_available():\n",
    "        model = torch.nn.DataParallel(_structure).cuda()\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(_structure)\n",
    "\n",
    "    return model\n",
    "\n",
    "#define which basic block we need to use; the nb of blocks per layer; nb of layers; output size(stucture)\n",
    "def resnet18_at(featureVectoreSize, **kwargs):\n",
    "    # Constructs base a ResNet-18 model.\n",
    "    model = ResNet_AT(BasicBlock, [2, 2, 2, 2],featureVectoreSize, **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "idfolder=0\n",
    "\n",
    "idpath=\"path\"+str(idfolder)\n",
    "folder=\"saveVids\"\n",
    "\n",
    "pathToSave=os.path.join(folder,idpath)\n",
    "\n",
    "if not os.path.exists(pathToSave):\n",
    "    os.makedirs(pathToSave)\n",
    "\n",
    "idImg=0\n",
    "\n",
    "\n",
    "#create the network\n",
    "model=resnet18_at(512)\n",
    "\n",
    "#load the network weights\n",
    "state_dict = torch.load('network_weigths.zip',map_location=torch.device('cpu'))\n",
    "#print(state_dict.keys())\n",
    "# create new OrderedDict that does not contain `module.`\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    type = k[:7] # remove `module.`\n",
    "    if(type=='module.'):\n",
    "        name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    #else:\n",
    "    #    new_state_dict[k] = v\n",
    "        \n",
    "model.load_state_dict(new_state_dict)\n",
    "print(\"model loading\")\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "#evaluating not training\n",
    "model.eval()\n",
    "\n",
    "transformValidation=transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
    "frames=[]\n",
    "\n",
    "good=0\n",
    "\n",
    "#add video name\n",
    "video_path=\"..\"\n",
    "\n",
    "#import pyautogui\n",
    "#myScreenshot = pyautogui.screenshot()\n",
    "#myScreenshot.save(r'ss1.png')\n",
    "import time\n",
    "time.sleep(10)\n",
    "print(\"start recording\")\n",
    "from PIL import ImageGrab \n",
    "im = ImageGrab.grab()\n",
    "width, height = im.size   # Get dimensions\n",
    "\n",
    "#left = (width)/4\n",
    "#top = (height)/4\n",
    "#right = (width)/4\n",
    "#bottom = (height)/4\n",
    "\n",
    "\n",
    "# Crop the center of the image\n",
    "#im = im.crop((left, top, right, bottom))\n",
    "#im.save('ss1.png')\n",
    "success=True\n",
    "\n",
    "\n",
    "#raise NameError(\"testing\")\n",
    "count = 0\n",
    "face_cascade = cv2.CascadeClassifier('detect.xml')\n",
    "\n",
    "\n",
    "while success:\n",
    "\n",
    "    #change the image to numpy array\n",
    "    frame = np.array(im)\n",
    "    print('Read a new frame: ', success)\n",
    "    count += 1\n",
    "    #crop, rotate and find the face of the image\n",
    "\n",
    "\n",
    "    try:\n",
    "        #revert to RGB\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        for (x, y, w, h) in faces:\n",
    "            print(\"face found\")\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "            faces = frame[y:y + h, x:x + w]\n",
    "            cv2.imwrite(\"saveimages/frame%d.jpg\" % count, faces)\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(\"face not found\")\n",
    "        continue\n",
    "    #raise nameError(\"found face stop\")\n",
    "    frame=cv2.cvtColor(faces, cv2.COLOR_BGR2RGB)\n",
    "    frame=transformValidation(Image.fromarray(frame))\n",
    "    frames.append(frame)\n",
    "    idImg+=1\n",
    "    good+=1\n",
    "    #take one frame of the video\n",
    "    im = ImageGrab.grab()\n",
    "    width, height = im.size   # Get dimensions\n",
    "\n",
    "    #left = (width)/4\n",
    "    #top = (height)/4\n",
    "    #right = (width)/4\n",
    "    #bottom = (height)/4\n",
    "\n",
    "    # Crop the center of the image\n",
    "    #im = im.crop((left, top, right, bottom))\n",
    "    #im.save('ss.png')\n",
    "    success=True\n",
    "    print(count)\n",
    "    if(count==3):\n",
    "        success=False\n",
    "good=0\n",
    "\n",
    "#no_grad: no training and no loss\n",
    "with torch.no_grad():\n",
    "    #convert from a python list to a tensor\n",
    "    input_var=torch.stack(frames, dim=3)\n",
    "    #add dimension (add a batch size of 1: for example 1*30*3*224*224)\n",
    "    input_var=input_var.unsqueeze(0)\n",
    "    #change input to GPU if exist\n",
    "    input_var = input_var.to(DEVICE)\n",
    "    #output of the network\n",
    "    pred_score = model(input_var)\n",
    "    #finding emotions based on max value\n",
    "    emotion=torch.argmax(pred_score).item()\n",
    "    #changing scores to probabilities\n",
    "    print(pred_score)\n",
    "    probabilities=F.softmax(pred_score, dim=1).cpu().numpy()\n",
    "\n",
    "print(probabilities)\n",
    "\n",
    "print(emotion)\n",
    "cat2Label={ \"Happy\": 0,\"Angry\": 1, \"Disgust\": 2, \"Fear\": 3, \"Sad\": 4, \"Neutral\": 5, \"Surprise\": 6, \"0\":\"happy\", \"1\":\"Angry\", \"2\":\"Disgust\", \"3\":\"Fear\", \"4\":\"Sad\", \"5\":\"Neutral\", \"6\":\"Surprise\"}\n",
    "print(cat2Label[str(emotion)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
